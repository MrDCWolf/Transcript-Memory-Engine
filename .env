# Database Configuration
DB_PATH=./data/transcripts.db

# Vector Store Configuration
CHROMA_PATH=./data/chroma_db
CHROMA_COLLECTION=transcripts

# LLM Configuration
OLLAMA_BASE_URL=http://host.docker.internal:11434 # For Mac Docker Desktop
# OLLAMA_BASE_URL=http://localhost:11434 # If running Ollama directly on host

# Ingestion Configuration (Example - Replace with actual values)
TRANSCRIPT_API_URL=http://example.com/api/transcripts
TRANSCRIPT_API_KEY=

# OpenAI Fallback (Optional)
OPENAI_API_KEY=