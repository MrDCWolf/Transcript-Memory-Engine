# Transcript Memory Engine: RAG Architecture

This document outlines the Retrieval-Augmented Generation (RAG) architecture implemented in the Transcript Memory Engine project. The goal is to enable users to chat with their stored conversation transcripts, asking questions and receiving contextually relevant answers generated by a Large Language Model (LLM).

The architecture prioritizes a local-first approach, utilizing local embedding models, a local vector database (ChromaDB), and local LLMs (via Ollama) by default.

## Key Components

*   **Transcript Database:** Stores raw transcript metadata and content (SQLite via `database/crud.py` and `database/models.py`).
*   **Chunker:** Splits transcripts into smaller, manageable text chunks (`ingest/chunker.py`).
*   **Embedding Model:** Generates vector representations (embeddings) for text chunks and user queries. Uses a local model (e.g., BGE) via Sentence Transformers by default (`embeddings/bge_local.py`, adheres to `interfaces.EmbeddingInterface`).
*   **Vector Store:** Stores chunk embeddings and allows for efficient similarity searches. Uses ChromaDB (`vector_stores/chroma_store.py`, adheres to `interfaces.VectorStoreInterface`).
*   **Retriever:** Takes a user query, embeds it, searches the vector store for relevant chunks, and potentially performs additional filtering or fetching (e.g., getting the latest transcript chunks) (`query/retriever.py`).
*   **LLM:** A large language model used for synthesizing answers based on the user query and retrieved context. Uses Ollama for local models by default (`llms/ollama_client.py`, adheres to `interfaces.LLMInterface`).
*   **RAG Service:** Orchestrates the entire RAG pipeline (`query/rag_service.py`).
*   **API:** Exposes the RAG functionality via a FastAPI endpoint (`api/routers/chat.py`).
*   **Interfaces:** Defines common contracts (`typing.Protocol`) for swappable components (`interfaces/`).

## Workflow

The RAG process involves two main phases: Ingestion and Querying.

### 1. Ingestion Phase

This phase processes transcripts and populates the vector store. It typically runs as a background process or script (`scripts/ingest.py` using `ingest.IngestionService`).

1.  **Fetch Transcripts:** Obtain new transcript data (details depend on the source, potentially via `ingest/fetcher.py`).
2.  **Store Transcript:** Save transcript metadata and raw content in the primary SQLite database (`database/crud.py`).
3.  **Chunk Transcript:** Divide the transcript content into smaller, overlapping chunks (`ingest/chunker.py`). Store these chunks, linked to the parent transcript, in the database.
4.  **Embed Chunks:** Generate vector embeddings for each chunk using the configured `EmbeddingInterface` implementation (e.g., `BGELocalEmbeddings`).
5.  **Store in Vector DB:** Add the chunks (content and metadata like `transcript_id`, timestamps) and their corresponding embeddings to the `VectorStoreInterface` implementation (e.g., `ChromaStore`). ChromaDB stores these locally.

### 2. Querying Phase (RAG Pipeline)

This phase handles user questions via the API (`api/routers/chat.py` calling `query.RAGService`).

1.  **Receive Query:** The user submits a question via the API.
2.  **(Optional) Direct DB Query:** Handle specific queries (like listing available dates) by directly querying the SQLite DB (`database/crud.py`).
3.  **Embed Query:** If it's a RAG query, embed the user's question using the `EmbeddingInterface` (`embed_query` method).
4.  **Retrieve Relevant Chunks:** Use the query embedding to search the `VectorStoreInterface` (`query` method) for the `k` most semantically similar chunks. The retriever (`query/retriever.py`) handles this.
5.  **Augment Context (Optional):**
    *   If the query involves recency (e.g., "today", "latest"), the `RAGService` explicitly fetches chunks from the most recent transcript(s) for that day from the SQLite DB (`database/crud.py`).
    *   These explicitly fetched chunks are merged with the semantically retrieved chunks.
    *   If recency is important, the combined list is sorted based on transcript timestamps.
6.  **Construct Prompt:**
    *   A system prompt (`SYSTEM_PROMPT_TEMPLATE` in `RAGService`) instructs the LLM on how to behave (synthesize information, rely solely on context, cite sources implicitly, handle missing info).
    *   The user's query is included.
    *   The content of the selected (retrieved and potentially augmented/sorted) chunks is added as context.
    *   **Token Budgeting:** The `RAGService` uses a tokenizer (`tiktoken`) to count tokens and dynamically selects the most relevant chunks (based on similarity score or recency sort order) that fit within the LLM's context window limit.
7.  **Generate Answer:** The structured prompt (including system message, query, and context) is sent to the configured `LLMInterface` (`chat` method, e.g., `OllamaClient`).
8.  **Return Response:** The LLM generates an answer based *only* on the provided context. This answer, along with metadata about the chunks used as context, is returned to the user via the API.

## Display

The final answer generated by the LLM is displayed to the user through the application's UI (currently a simple Streamlit interface via `ui/app.py`, accessed via the FastAPI backend). The API also returns the specific chunks used to generate the answer, which can optionally be displayed for transparency or further interaction. 